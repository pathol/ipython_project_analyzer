{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbformat import reads, NO_CONVERT\n",
    "from nbconvert import PythonExporter\n",
    "def code_extractor(jpt):\n",
    "    nb = reads(jpt, NO_CONVERT)\n",
    "    exporter = PythonExporter()\n",
    "    source, meta = exporter.from_notebook_node(nb)\n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "Matchmaker = imp.load_source('name', '../matchmaker.py')\n",
    "import ast\n",
    "def function_matching(nb):\n",
    "    tree = ast.parse(nb)\n",
    "    mm = Matchmaker.Matchmaker()\n",
    "    mm.visit(tree)\n",
    "    matchs = mm.matching()\n",
    "    return matchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining, GitRepository\n",
    "import difflib\n",
    "import pandas\n",
    "\n",
    "def repo_matching(repo, commit, path):\n",
    "    gr = GitRepository(repo)\n",
    "    for c in RepositoryMining(repo, single = commit).traverse_commits():\n",
    "        for modified_files in commit.modifications:\n",
    "            if modified_files.new_path == path:\n",
    "                before = code_extractor(modified_files.source_code_before)\n",
    "                lines_b = before.strip().splitlines()\n",
    "                new = code_extractor(modified_files.source_code)\n",
    "                lines_n = new.strip().splitlines()\n",
    "                \n",
    "                diff = difflib.unified_diff(lines_b, lines_n, fromfile='before', tofile='new', lineterm='', n=0)\n",
    "                lines = list(diff)[2:]\n",
    "                lineno = 0\n",
    "                changes = pd.DataFrame(columns = ['repo','path','author','commit','line','code'])\n",
    "                for line in lines:\n",
    "                    prefix = '@@'\n",
    "                    if line.startswith(prefix):\n",
    "                        s = line[line.find(\"+\"):]\n",
    "                        try:\n",
    "                            lineno = int(s[1:s.find(\",\")])\n",
    "                        except:\n",
    "                            lineno = int(s[1:s.find(\" \")])\n",
    "                    else:\n",
    "                        if line.startswith(\"+\"):\n",
    "                            changes = changes.append({\"repo\": c.project_name, \n",
    "                                                      \"path\": modified_files.new_path,\n",
    "                                                      \"author\": c.author.email,\n",
    "                                                      \"commit\": c.hash,\n",
    "                                                      \"line\": lineno,\n",
    "                                                      \"code\": line}, ignore_index= True)\n",
    "                            lineno += 1\n",
    "                matches = function_matching(new)\n",
    "                result = pd.merge(matches, changes, how = 'inner', on= \"line\")\n",
    "                return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "# incresing the csv field size\n",
    "import ctypes\n",
    "csv.field_size_limit(int(ctypes.c_ulong(-1).value // 2))\n",
    "failed = []\n",
    "fields = ['lineno','line','function','package','class','repo','file', 'author']\n",
    "repo_path = \"\"\n",
    "with open('../data/commits_bigquery.csv',\"r\", encoding=\"utf8\") as csvfile:\n",
    "    with open(\"../data/repo_function_matched.csv\", \"w\", encoding=\"utf8\") as newfile:\n",
    "        data = csv.DictReader(csvfile)\n",
    "        writer = csv.DictWriter(newfile, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            try:\n",
    "                commit = row['commit']\n",
    "                repo = row['repo']\n",
    "                path = row['path']\n",
    "                path = path.replace(\"/\", \"\\\\\")\n",
    "                matched = repo_matching(repo, commit, path)\n",
    "                matched.to_csv(newfile, header=False, index= False)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except Exception as e: \n",
    "                failed.append(e)\n",
    "                continue\n",
    "import pickle\n",
    "\n",
    "with open('../data/fails', 'wb') as fp:\n",
    "    pickle.dump(failed, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
