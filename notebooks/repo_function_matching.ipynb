{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "def repo_matching(repo, commit, path):\n",
    "    for c in RepositoryMining(repo, single = commit).traverse_commits():\n",
    "        for modified_files in c.modifications:\n",
    "            if modified_files.new_path == path:\n",
    "                try:\n",
    "                    before = code_extractor(modified_files.source_code_before)\n",
    "                except TypeError:\n",
    "                    before = \"\"\n",
    "                lines_b = before.strip().splitlines()\n",
    "                new = code_extractor(modified_files.source_code)\n",
    "                lines_n = new.strip().splitlines()\n",
    "                \n",
    "                diff = difflib.unified_diff(lines_b, lines_n, fromfile='before', tofile='new', lineterm='', n=0)\n",
    "                lines = list(diff)[2:]\n",
    "                lineno = 0\n",
    "                changes = pd.DataFrame(columns = ['repo','path','author','commit','line','code'])\n",
    "                for line in lines:\n",
    "                    prefix = '@@'\n",
    "                    if line.startswith(prefix):\n",
    "                        s = line[line.find(\"+\"):]\n",
    "                        try:\n",
    "                            lineno = int(s[1:s.find(\",\")])\n",
    "                        except:\n",
    "                            lineno = int(s[1:s.find(\" \")])\n",
    "                    else:\n",
    "                        if line.startswith(\"+\"):\n",
    "                            changes = changes.append({\"repo\": c.project_name, \n",
    "                                                      \"path\": modified_files.new_path,\n",
    "                                                      \"author\": c.author.email,\n",
    "                                                      \"commit\": c.hash,\n",
    "                                                      \"line\": lineno,\n",
    "                                                      \"code\": line[1:]}, ignore_index= True)\n",
    "                            lineno += 1\n",
    "                matches = function_matching(new)\n",
    "                result = pd.merge(matches, changes, how = 'inner', on= \"line\")\n",
    "                return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining, GitRepository\n",
    "repo = \"/home/ubuntu/repos/abulbasar/machine-learning\"\n",
    "for commit in RepositoryMining(repo).traverse_commits():\n",
    "    for file in commit.modifications:\n",
    "        print(commit.hash + ' at ' + file.new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"/home/ubuntu/repos/abulbasar/machine-learning\"\n",
    "path = \"01 Neural Network using Numpy.ipynb\"\n",
    "commit = \"df41ae028af5a445ec41a94925ac98ad2ccdea01\"\n",
    "\n",
    "m = repo_matching(repo = repo,path = path,commit = commit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running through all commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "# incresing the csv field size\n",
    "import ctypes\n",
    "csv.field_size_limit(int(ctypes.c_ulong(-1).value // 2))\n",
    "failed = []\n",
    "fields = ['line','function','package','class','repo','path', 'author', 'commit','code']\n",
    "repo_path = \"/home/ubuntu/repos/\"\n",
    "with open('../data/commits_bigquery.csv',\"r\", encoding=\"utf8\") as csvfile:\n",
    "    with open(\"../data/repo_function_matched.csv\", \"w\", encoding=\"utf8\") as newfile:\n",
    "        data = csv.DictReader(csvfile)\n",
    "        writer = csv.DictWriter(newfile, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            try:\n",
    "                commit = row['commit']\n",
    "                repo = row['repo']\n",
    "                path = row['path']\n",
    "                path = path.replace(\"/\", \"\\\\\")\n",
    "                repo = repo_path + repo\n",
    "                matched = repo_matching(repo, commit, path)\n",
    "                matched.to_csv(newfile, header=False, index= False)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except Exception as e: \n",
    "                failed.append(e)\n",
    "                #print(e)\n",
    "                #break\n",
    "import pickle\n",
    "\n",
    "with open('../data/fails', 'wb') as fp:\n",
    "    pickle.dump(failed, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/commits_bigquery.csv')\n",
    "df = df.drop(df.columns[0], axis = 1)\n",
    "repos = df.repo.unique()\n",
    "add_path = np.vectorize(lambda x: \"/home/ubuntu/repos/\" +x)\n",
    "repos = add_path(repos)\n",
    "repos = repos.tolist()\n",
    "commits = df.commit.unique().tolist()\n",
    "paths = df[['commit','path']].groupby('commit')['path'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all changes and write them to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbformat import reads, NO_CONVERT\n",
    "from nbconvert import PythonExporter\n",
    "def code_extractor(jpt):\n",
    "    nb = reads(jpt, NO_CONVERT)\n",
    "    exporter = PythonExporter()\n",
    "    source, meta = exporter.from_notebook_node(nb)\n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "Matchmaker = imp.load_source('name', '../matchmaker.py')\n",
    "import ast\n",
    "def function_matching(nb):\n",
    "    tree = ast.parse(nb)\n",
    "    mm = Matchmaker.Matchmaker()\n",
    "    mm.visit(tree)\n",
    "    matchs = mm.matching()\n",
    "    return matchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing parentheses in call to 'print' (<unknown>, line 29)\n",
      "Missing parentheses in call to 'print' (<unknown>, line 41)\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "Missing parentheses in call to 'print' (<unknown>, line 29)\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "Notebook does not appear to be JSON: '{\\n  \"worksheets\": [\\n    {\\n      \"cel...\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "cells\n",
      "Missing parentheses in call to 'print' (<unknown>, line 42)\n",
      "cells\n",
      "Missing parentheses in call to 'print' (<unknown>, line 43)\n",
      "Missing parentheses in call to 'print' (<unknown>, line 43)\n",
      "Missing parentheses in call to 'print' (<unknown>, line 43)\n",
      "Missing parentheses in call to 'print' (<unknown>, line 43)\n",
      "Missing parentheses in call to 'print' (<unknown>, line 44)\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "Missing parentheses in call to 'print' (<unknown>, line 44)\n",
      "'Tuple' object has no attribute 'id'\n",
      "Missing parentheses in call to 'print' (<unknown>, line 45)\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "Missing parentheses in call to 'print' (<unknown>, line 107)\n",
      "Missing parentheses in call to 'print' (<unknown>, line 107)\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "Missing parentheses in call to 'print' (<unknown>, line 29)\n",
      "Missing parentheses in call to 'print' (<unknown>, line 41)\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "Missing parentheses in call to 'print' (<unknown>, line 41)\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "'Tuple' object has no attribute 'id'\n",
      "invalid syntax (<unknown>, line 79)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-6c2ff0713d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mRepositoryMining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_no_merge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_commits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraverse_commits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodified_files\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodifications\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pydriller/repository_mining.py\u001b[0m in \u001b[0;36mtraverse_commits\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m                             commit.author.name)\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_commit_filtered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Commit #%s filtered'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pydriller/repository_mining.py\u001b[0m in \u001b[0;36m_is_commit_filtered\u001b[0;34m(self, commit)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_only_commits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcommit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_only_commits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             logger.debug(\"Commit filtered because it is not one of the \"\n\u001b[1;32m    247\u001b[0m                          \"specified commits\")\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pydriller import RepositoryMining\n",
    "import difflib\n",
    "import pandas as pd\n",
    "#inspect all repos and commits\n",
    "failed = []\n",
    "fields = ['line','function','package','class','repo','path', 'author', 'commit','code']\n",
    "newfile = open(\"../data/repo_function_matched.csv\", \"w\", encoding=\"utf8\")\n",
    "writer = csv.DictWriter(newfile, fieldnames=fields)\n",
    "writer.writeheader()\n",
    "    \n",
    "for c in RepositoryMining(repos, only_no_merge=True, only_commits=commits).traverse_commits():\n",
    "    for modified_files in c.modifications:\n",
    "        try:\n",
    "            if modified_files.new_path in paths[c.hash]:\n",
    "                try:\n",
    "                    before = code_extractor(modified_files.source_code_before)\n",
    "                except TypeError:\n",
    "                    before = \"\"\n",
    "                lines_b = before.strip().splitlines()\n",
    "                new = code_extractor(modified_files.source_code)\n",
    "                lines_n = new.strip().splitlines()\n",
    "                \n",
    "                diff = difflib.unified_diff(lines_b, lines_n, fromfile='before', tofile='new', lineterm='', n=0)\n",
    "                lines = list(diff)[2:]\n",
    "                lineno = 0\n",
    "                changes = pd.DataFrame(columns = ['repo','path','author','commit','line','code'])\n",
    "                for line in lines:\n",
    "                    prefix = '@@'\n",
    "                    if line.startswith(prefix):\n",
    "                        s = line[line.find(\"+\"):]\n",
    "                        try:\n",
    "                            lineno = int(s[1:s.find(\",\")])\n",
    "                        except:\n",
    "                            lineno = int(s[1:s.find(\" \")])\n",
    "                    else:\n",
    "                        if line.startswith(\"+\"):\n",
    "                            changes = changes.append({\"repo\": c.project_name, \n",
    "                                                      \"path\": modified_files.new_path,\n",
    "                                                      \"author\": c.author.email,\n",
    "                                                      \"commit\": c.hash,\n",
    "                                                      \"line\": lineno,\n",
    "                                                      \"code\": line[1:]}, ignore_index= True)\n",
    "                            lineno += 1\n",
    "                matches = function_matching(new)\n",
    "                result = pd.merge(matches, changes, how = 'inner', on= \"line\")\n",
    "                result.to_csv(newfile, header=False, index= False)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e: \n",
    "            failed.append([c.project_name,modified_files.new_path,e])\n",
    "\n",
    "newfile.close()\n",
    "                \n",
    "import pickle\n",
    "\n",
    "with open('../data/fails', 'wb') as fp:\n",
    "    pickle.dump(failed, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
