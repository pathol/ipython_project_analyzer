{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbformat import reads, NO_CONVERT\n",
    "from nbconvert import PythonExporter\n",
    "def code_extractor(jpt):\n",
    "    nb = reads(jpt, NO_CONVERT)\n",
    "    exporter = PythonExporter()\n",
    "    source, meta = exporter.from_notebook_node(nb)\n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "Matchmaker = imp.load_source('name', '../matchmaker.py')\n",
    "import ast\n",
    "def function_matching(nb):\n",
    "    tree = ast.parse(nb)\n",
    "    mm = Matchmaker.Matchmaker()\n",
    "    mm.visit(tree)\n",
    "    matchs = mm.matching()\n",
    "    return matchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching the functions 1st version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "def repo_matching(repo, commit, path):\n",
    "    for c in RepositoryMining(repo, single = commit).traverse_commits():\n",
    "        for modified_files in c.modifications:\n",
    "            if modified_files.new_path == path:\n",
    "                try:\n",
    "                    before = code_extractor(modified_files.source_code_before)\n",
    "                except TypeError:\n",
    "                    before = \"\"\n",
    "                lines_b = before.strip().splitlines()\n",
    "                new = code_extractor(modified_files.source_code)\n",
    "                lines_n = new.strip().splitlines()\n",
    "                \n",
    "                diff = difflib.unified_diff(lines_b, lines_n, fromfile='before', tofile='new', lineterm='', n=0)\n",
    "                lines = list(diff)[2:]\n",
    "                lineno = 0\n",
    "                changes = pd.DataFrame(columns = ['repo','path','author','commit','line','code'])\n",
    "                for line in lines:\n",
    "                    prefix = '@@'\n",
    "                    if line.startswith(prefix):\n",
    "                        s = line[line.find(\"+\"):]\n",
    "                        try:\n",
    "                            lineno = int(s[1:s.find(\",\")])\n",
    "                        except:\n",
    "                            lineno = int(s[1:s.find(\" \")])\n",
    "                    else:\n",
    "                        if line.startswith(\"+\"):\n",
    "                            changes = changes.append({\"repo\": c.project_name, \n",
    "                                                      \"path\": modified_files.new_path,\n",
    "                                                      \"author\": c.author.email,\n",
    "                                                      \"commit\": c.hash,\n",
    "                                                      \"line\": lineno,\n",
    "                                                      \"code\": line[1:]}, ignore_index= True)\n",
    "                            lineno += 1\n",
    "                matches = function_matching(new)\n",
    "                result = pd.merge(matches, changes, how = 'inner', on= \"line\")\n",
    "                return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining, GitRepository\n",
    "repo = \"/home/ubuntu/repos/abulbasar/machine-learning\"\n",
    "test = RepositoryMining(repo)\n",
    "for commit in test.traverse_commits():\n",
    "    for file in commit.modifications:\n",
    "        print(commit.hash + ' at ' + file.new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"/home/ubuntu/repos/abulbasar/machine-learning\"\n",
    "path = \"01 Neural Network using Numpy.ipynb\"\n",
    "commit = \"df41ae028af5a445ec41a94925ac98ad2ccdea01\"\n",
    "\n",
    "m = repo_matching(repo = repo,path = path,commit = commit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running through all commits 1st version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "# incresing the csv field size\n",
    "import ctypes\n",
    "csv.field_size_limit(int(ctypes.c_ulong(-1).value // 2))\n",
    "failed = []\n",
    "fields = ['line','function','package','class','repo','path', 'author', 'commit','code']\n",
    "repo_path = \"/home/ubuntu/repos/\"\n",
    "with open('../data/commits_bigquery.csv',\"r\", encoding=\"utf8\") as csvfile:\n",
    "    with open(\"../data/repo_function_matched.csv\", \"w\", encoding=\"utf8\") as newfile:\n",
    "        data = csv.DictReader(csvfile)\n",
    "        writer = csv.DictWriter(newfile, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            try:\n",
    "                commit = row['commit']\n",
    "                repo = row['repo']\n",
    "                path = row['path']\n",
    "                #path = path.replace(\"/\", \"\\\\\")\n",
    "                repo = repo_path + repo\n",
    "                matched = repo_matching(repo, commit, path)\n",
    "                matched.to_csv(newfile, header=False, index= False)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except Exception as e: \n",
    "                failed.append(e)\n",
    "                #print(e)\n",
    "                #break\n",
    "import pickle\n",
    "\n",
    "with open('../data/fails', 'wb') as fp:\n",
    "    pickle.dump(failed, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data 2nd version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "df = pd.read_csv('../data/commits_bigquery.csv')\n",
    "df = df.drop(df.columns[0], axis = 1)\n",
    "\n",
    "repos = df.repo.unique()\n",
    "add_path = np.vectorize(lambda x: \"/home/ubuntu/repos/\" +x)\n",
    "repos = add_path(repos)\n",
    "repos = repos.tolist()\n",
    "for i in repos:\n",
    "    if not path.exists(i):\n",
    "        repos.remove(i)\n",
    "\n",
    "commits = df.commit.unique().tolist()\n",
    "\n",
    "paths = df[['commit','path']].groupby('commit')['path'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import csv\n",
    "#inspect all repos and commits\n",
    "failed = []\n",
    "fields = ['line','function','package','class','repo','path', 'author', 'commit','code']\n",
    "newfile = open(\"../data/repo_function_matched.csv\", \"w\", encoding=\"utf8\")\n",
    "writer = csv.DictWriter(newfile, fieldnames=fields)\n",
    "writer.writeheader()\n",
    "    \n",
    "for c in RepositoryMining(repos, only_no_merge=True, only_commits=commits).traverse_commits():\n",
    "    for modified_files in c.modifications:\n",
    "        try:\n",
    "            if modified_files.new_path in paths[c.hash]:\n",
    "                try:\n",
    "                    before = code_extractor(modified_files.source_code_before)\n",
    "                except TypeError:\n",
    "                    before = \"\"\n",
    "                lines_b = before.strip().splitlines()\n",
    "                new = code_extractor(modified_files.source_code)\n",
    "                lines_n = new.strip().splitlines()\n",
    "                \n",
    "                diff = difflib.unified_diff(lines_b, lines_n, fromfile='before', tofile='new', lineterm='', n=0)\n",
    "                lines = list(diff)[2:]\n",
    "                lineno = 0\n",
    "                changes = pd.DataFrame(columns = ['repo','path','author','commit','line','code'])\n",
    "                for line in lines:\n",
    "                    prefix = '@@'\n",
    "                    if line.startswith(prefix):\n",
    "                        s = line[line.find(\"+\"):]\n",
    "                        try:\n",
    "                            lineno = int(s[1:s.find(\",\")])\n",
    "                        except:\n",
    "                            lineno = int(s[1:s.find(\" \")])\n",
    "                    else:\n",
    "                        if line.startswith(\"+\"):\n",
    "                            changes = changes.append({\"repo\": c.project_name, \n",
    "                                                      \"path\": modified_files.new_path,\n",
    "                                                      \"author\": c.author.email,\n",
    "                                                      \"commit\": c.hash,\n",
    "                                                      \"line\": lineno,\n",
    "                                                      \"code\": line[1:]}, ignore_index= True)\n",
    "                            lineno += 1\n",
    "                matches = function_matching(new)\n",
    "                result = pd.merge(matches, changes, how = 'inner', on= \"line\")\n",
    "                result.to_csv(newfile, header=False, index= False)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e: \n",
    "            failed.append([c.project_name,modified_files.new_path,e])\n",
    "\n",
    "newfile.close()\n",
    "                \n",
    "import pickle\n",
    "\n",
    "with open('../data/fails', 'wb') as fp:\n",
    "    pickle.dump(failed, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data 3rd version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "df = pd.read_csv('../data/commits_bigquery.csv')\n",
    "df = df.drop(df.columns[0], axis = 1)\n",
    "\n",
    "repos = df.repo.unique()\n",
    "folder = \"/home/ubuntu/repos/\"\n",
    "repos = repos.tolist()\n",
    "for i in repos:\n",
    "    if not path.exists(folder+i):\n",
    "        repos.remove(i)\n",
    "repos = repos.tolist()\n",
    "folder = \"/home/ubuntu/repos/\"\n",
    "commits = df[['repo','commit']].groupby('repo')['commit'].apply(list)\n",
    "paths = df[['commit','path']].groupby('commit')['path'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running 3rd version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import csv\n",
    "#inspect all repos and commits\n",
    "failed = []\n",
    "fields = ['line','function','package','class','repo','path', 'author', 'commit','code']\n",
    "newfile = open(\"../data/repo_function_matched.csv\", \"w\", encoding=\"utf8\")\n",
    "writer = csv.DictWriter(newfile, fieldnames=fields)\n",
    "writer.writeheader()\n",
    "    \n",
    "for r in repos:\n",
    "    try:\n",
    "        final_path = folder+r\n",
    "        repository = RepositoryMining(final_path, only_no_merge=True, only_commits=commits[r])\n",
    "        for c in repository.traverse_commits():\n",
    "            for f in c.modifications:\n",
    "                try:\n",
    "                    if f.new_path in paths[c.hash]:\n",
    "                        try:\n",
    "                            before = code_extractor(f.source_code_before)\n",
    "                        except TypeError:\n",
    "                            before = \"\"\n",
    "                        lines_b = before.strip().splitlines()\n",
    "                        new = code_extractor(f.source_code)\n",
    "                        lines_n = new.strip().splitlines()\n",
    "                        \n",
    "                        diff = difflib.unified_diff(lines_b, lines_n, fromfile='before', tofile='new', lineterm='', n=0)\n",
    "                        lines = list(diff)[2:]\n",
    "                        lineno = 0\n",
    "                        changes = pd.DataFrame(columns = ['repo','path','author','commit','line','code'])\n",
    "                        for line in lines:\n",
    "                            prefix = '@@'\n",
    "                            if line.startswith(prefix):\n",
    "                                s = line[line.find(\"+\"):]\n",
    "                                try:\n",
    "                                    lineno = int(s[1:s.find(\",\")])\n",
    "                                except:\n",
    "                                    lineno = int(s[1:s.find(\" \")])\n",
    "                            else:\n",
    "                                if line.startswith(\"+\"):\n",
    "                                    changes = changes.append({\"repo\": c.project_name, \n",
    "                                                              \"path\": f.new_path,\n",
    "                                                              \"author\": c.author.email,\n",
    "                                                              \"commit\": c.hash,\n",
    "                                                              \"line\": lineno,\n",
    "                                                              \"code\": line[1:]}, ignore_index= True)\n",
    "                                    lineno += 1\n",
    "                        matches = function_matching(new)\n",
    "                        result = pd.merge(matches, changes, how = 'inner', on= \"line\")\n",
    "                        result.to_csv(newfile, header=False, index= False)\n",
    "                        del result\n",
    "                        del changes\n",
    "                        del matches\n",
    "                except KeyboardInterrupt:\n",
    "                    break\n",
    "                except Exception as e: \n",
    "                    failed.append(['File',f.new_path,e])\n",
    "        del repository\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except Exception as e: \n",
    "        failed.append(['Project',r,e])\n",
    "\n",
    "newfile.close()\n",
    "                \n",
    "import pickle\n",
    "\n",
    "with open('../data/fails', 'wb') as fp:\n",
    "    pickle.dump(failed, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running 4th version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_matcher(c,f,newfile):\n",
    "    if f.new_path in paths[c.hash]:\n",
    "        try:\n",
    "            before = code_extractor(f.source_code_before)\n",
    "        except TypeError:\n",
    "            before = \"\"\n",
    "        lines_b = before.strip().splitlines()\n",
    "        new = code_extractor(f.source_code)\n",
    "        lines_n = new.strip().splitlines()\n",
    "        \n",
    "        diff = difflib.unified_diff(lines_b, lines_n, fromfile='before', tofile='new', lineterm='', n=0)\n",
    "        lines = list(diff)[2:]\n",
    "        lineno = 0\n",
    "        changes = pd.DataFrame(columns = ['repo','path','author','commit','line','code'])\n",
    "        for line in lines:\n",
    "            prefix = '@@'\n",
    "            if line.startswith(prefix):\n",
    "                s = line[line.find(\"+\"):]\n",
    "                try:\n",
    "                    lineno = int(s[1:s.find(\",\")])\n",
    "                except:\n",
    "                    lineno = int(s[1:s.find(\" \")])\n",
    "            else:\n",
    "                if line.startswith(\"+\"):\n",
    "                    changes = changes.append({\"repo\": c.project_name, \n",
    "                                              \"path\": f.new_path,\n",
    "                                              \"author\": c.author.email,\n",
    "                                              \"commit\": c.hash,\n",
    "                                              \"line\": lineno,\n",
    "                                              \"code\": line[1:]}, ignore_index= True)\n",
    "                    lineno += 1\n",
    "        matches = function_matching(new)\n",
    "        result = pd.merge(matches, changes, how = 'inner', on= \"line\")\n",
    "        result.to_csv(newfile, mode='a', header=False, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repo_matcher(repo, commits, paths, newfile):\n",
    "    final_path = folder+repo\n",
    "    repository = RepositoryMining(final_path, only_no_merge=True, only_commits=commits[r])\n",
    "    for c in repository.traverse_commits():\n",
    "        for f in c.modifications:\n",
    "            try:\n",
    "                file_matcher(c,f,newfile)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except: \n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "df = pd.read_csv('../data/commits_bigquery.csv')\n",
    "df = df.drop(df.columns[0], axis = 1)\n",
    "\n",
    "repos = df.repo.unique()\n",
    "folder = \"/home/ubuntu/repos/\"\n",
    "repos = repos.tolist()\n",
    "for i in repos:\n",
    "    if not path.exists(folder+i):\n",
    "        repos.remove(i)\n",
    "\n",
    "commits = df[['repo','commit']].groupby('repo')['commit'].apply(list)\n",
    "paths = df[['commit','path']].groupby('commit')['path'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import csv\n",
    "#inspect all repos and commits\n",
    "fields = ['line','function','package','class','repo','path', 'author', 'commit','code']\n",
    "to_disk = \"../data/repo_function_matched.csv\"\n",
    "with open(to_disk, \"w\", encoding=\"utf8\") as newfile:\n",
    "    writer = csv.DictWriter(newfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    \n",
    "for r in repos:\n",
    "    try:\n",
    "        repo_matcher(r, commits, paths, to_disk)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5th Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbformat import reads, NO_CONVERT\n",
    "from nbconvert import PythonExporter\n",
    "def code_extractor(jpt):\n",
    "    nb = reads(jpt, NO_CONVERT)\n",
    "    exporter = PythonExporter()\n",
    "    source, meta = exporter.from_notebook_node(nb)\n",
    "    return stripComments(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def stripComments(code):\n",
    "    code = str(code)\n",
    "    code = re.sub(r'^$\\n', '', code, flags=re.MULTILINE)\n",
    "    return re.sub(r'(?m)^ *#.*\\n?', '', code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "Matchmaker = imp.load_source('name', '../matchmaker.py')\n",
    "import ast\n",
    "def function_matching(nb):\n",
    "    tree = ast.parse(nb)\n",
    "    mm = Matchmaker.Matchmaker()\n",
    "    mm.visit(tree)\n",
    "    matchs = mm.matching()\n",
    "    return matchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_matcher(c,f,newfile):\n",
    "    if f.new_path in paths[c.hash]:\n",
    "        try:\n",
    "            before = code_extractor(f.source_code_before)\n",
    "        except TypeError:\n",
    "            before = \"\"\n",
    "        lines_b = before.strip().splitlines()\n",
    "        new = code_extractor(f.source_code)\n",
    "        lines_n = new.strip().splitlines()\n",
    "        \n",
    "        columns = ['repo','path','author','commit','line','code']\n",
    "        temp_path = \"../data/temp.csv\"\n",
    "        with open(temp_path, \"w\", encoding=\"utf8\") as temp:\n",
    "            writer = csv.DictWriter(temp, fieldnames=fields)\n",
    "            writer.writeheader()\n",
    "            if before != \"\":\n",
    "                diff = difflib.unified_diff(lines_b, lines_n, fromfile='before', tofile='new', lineterm='', n=0)\n",
    "                lines = list(diff)[2:]\n",
    "                lineno = 0\n",
    "                for line in lines:\n",
    "                    prefix = '@@'\n",
    "                    if line.startswith(prefix):\n",
    "                        s = line[line.find(\"+\"):]\n",
    "                        try:\n",
    "                            lineno = int(s[1:s.find(\",\")])\n",
    "                        except:\n",
    "                            lineno = int(s[1:s.find(\" \")])\n",
    "                    else:\n",
    "                        if line.startswith(\"+\"):\n",
    "                            writer.writerow({\"repo\": c.project_name, \n",
    "                                             \"path\": f.new_path,\n",
    "                                             \"author\": c.author.email,\n",
    "                                             \"commit\": c.hash,\n",
    "                                             \"line\": lineno,\n",
    "                                             \"code\": line[1:]})\n",
    "                            lineno += 1\n",
    "            else:\n",
    "                for idx,line in enumerate(lines_n):\n",
    "                    writer.writerow({\"repo\": c.project_name, \n",
    "                                     \"path\": f.new_path,\n",
    "                                     \"author\": c.author.email,\n",
    "                                     \"commit\": c.hash,\n",
    "                                     \"line\": idx+1,\n",
    "                                     \"code\": line})\n",
    "            \n",
    "                \n",
    "        changes = pd.read_csv(temp_path)\n",
    "        changes['line'] = changes['line'].astype(int)\n",
    "        matches = function_matching(new)\n",
    "        matches['line'] = matches['line'].astype(int)\n",
    "        result = pd.merge(matches, changes, how = 'inner', on= \"line\")\n",
    "        result.to_csv(newfile, mode='a', header=False, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repo_matcher(repo, commits, paths, newfile):\n",
    "    final_path = folder+repo\n",
    "    repository = RepositoryMining(final_path, only_no_merge=True, only_commits=commits[r])\n",
    "    for c in repository.traverse_commits():\n",
    "        for f in c.modifications:\n",
    "            try:\n",
    "                file_matcher(c,f,newfile)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except: \n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "df = pd.read_csv('../data/commits_bigquery.csv')\n",
    "df = df.drop(df.columns[0], axis = 1)\n",
    "data = pd.read_csv('../data/repo_function_matched_v4.csv')\n",
    "un = data[['path','commit']].drop_duplicates()\n",
    "\n",
    "df_all = df.merge(un, on=['path','commit'], how='left', indicator=True)\n",
    "df_all = df_all[df_all['_merge'] == 'left_only']\n",
    "\n",
    "repos = df_all.repo.unique()\n",
    "folder = \"/home/ubuntu/repos/\"\n",
    "repos = repos.tolist()\n",
    "for i in repos:\n",
    "    if not path.exists(folder+i):\n",
    "        repos.remove(i)\n",
    "\n",
    "\n",
    "commits = df_all[['repo','commit']].groupby('repo')['commit'].apply(list)\n",
    "paths = df_all[['commit','path']].groupby('commit')['path'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import csv\n",
    "#inspect all repos and commits\n",
    "to_disk = \"../data/repo_function_matched.csv\"\n",
    "    \n",
    "for r in repos:\n",
    "    try:\n",
    "        repo_matcher(r, commits, paths, to_disk)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except: \n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
