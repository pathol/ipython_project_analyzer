{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all Commit Changes and match the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove comments and empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def stripComments(code):\n",
    "    code = str(code)\n",
    "    code = re.sub(r'^$\\n', '', code, flags=re.MULTILINE)\n",
    "    return re.sub(r'(?m)^ *#.*\\n?', '', code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert ipython to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbformat import reads, NO_CONVERT\n",
    "from nbconvert import PythonExporter\n",
    "def code_extractor(jpt):\n",
    "    nb = reads(jpt, NO_CONVERT)\n",
    "    exporter = PythonExporter()\n",
    "    source, meta = exporter.from_notebook_node(nb)\n",
    "    return stripComments(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calls Matchmaker and matches functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "Matchmaker = imp.load_source('name', '../matchmaker.py')\n",
    "import ast\n",
    "def function_matching(nb):\n",
    "    tree = ast.parse(nb)\n",
    "    mm = Matchmaker.Matchmaker()\n",
    "    mm.visit(tree)\n",
    "    matchs = mm.matching()\n",
    "    return matchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for a file in a commit convert ipython to python, match the functions and write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import pandas as pd\n",
    "def file_matcher(c,f,newfile):\n",
    "    if f.new_path in paths[c.hash]:\n",
    "        try:\n",
    "            before = code_extractor(f.source_code_before)\n",
    "        except TypeError:\n",
    "            before = \"\"\n",
    "        lines_b = before.strip().splitlines()\n",
    "        new = code_extractor(f.source_code)\n",
    "        lines_n = new.strip().splitlines()\n",
    "        \n",
    "        matches = function_matching(new)\n",
    "        \n",
    "        columns = ['repo','path','author','commit','line','code']\n",
    "        temp_path = \"../data/temp.csv\"\n",
    "        with open(temp_path, \"w\", encoding=\"utf8\") as temp:\n",
    "            writer = csv.DictWriter(temp, fieldnames=columns)\n",
    "            writer.writeheader()\n",
    "            if before != \"\":\n",
    "                diff = difflib.unified_diff(lines_b, lines_n, fromfile='before', tofile='new', lineterm='', n=0)\n",
    "                lines = list(diff)[2:]\n",
    "                lineno = 0\n",
    "                for line in lines:\n",
    "                    prefix = '@@'\n",
    "                    if line.startswith(prefix):\n",
    "                        s = line[line.find(\"+\"):]\n",
    "                        try:\n",
    "                            lineno = int(s[1:s.find(\",\")])\n",
    "                        except:\n",
    "                            lineno = int(s[1:s.find(\" \")])\n",
    "                    else:\n",
    "                        if line.startswith(\"+\"):\n",
    "                            writer.writerow({\"repo\": c.project_name, \n",
    "                                             \"path\": f.new_path,\n",
    "                                             \"author\": c.author.email,\n",
    "                                             \"commit\": c.hash,\n",
    "                                             \"line\": lineno,\n",
    "                                             \"code\": line[1:]})\n",
    "                            lineno += 1\n",
    "            else:\n",
    "                for idx,line in enumerate(lines_n):\n",
    "                    writer.writerow({\"repo\": c.project_name, \n",
    "                                     \"path\": f.new_path,\n",
    "                                     \"author\": c.author.email,\n",
    "                                     \"commit\": c.hash,\n",
    "                                     \"line\": idx+1,\n",
    "                                     \"code\": line})\n",
    "      \n",
    "        changes = pd.read_csv(temp_path)\n",
    "        changes['line'] = changes['line'].astype(int)\n",
    "        matches['line'] = matches['line'].astype(int)\n",
    "        result = pd.merge(matches, changes, how = 'inner', on= \"line\")\n",
    "        result.to_csv(newfile, mode='a', header=False, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the repository locally stored and iterate through commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repo_matcher(repo, commits, paths, newfile):\n",
    "    final_path = folder+repo\n",
    "    repository = RepositoryMining(final_path, only_no_merge=True, only_commits=commits[r])\n",
    "    for c in repository.traverse_commits():\n",
    "        for f in c.modifications:\n",
    "            try:\n",
    "                file_matcher(c,f,newfile)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load commit data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "df = pd.read_csv('../data/commits_bigquery.csv')\n",
    "df = df.drop(df.columns[0], axis = 1)\n",
    "\n",
    "repos = df_all.repo.unique()\n",
    "folder = \"/home/ubuntu/repos/\"\n",
    "repos = repos.tolist()\n",
    "for i in repos:\n",
    "    if not path.exists(folder+i):\n",
    "        repos.remove(i)\n",
    "\n",
    "\n",
    "commits = df_all[['repo','commit']].groupby('repo')['commit'].apply(list)\n",
    "paths = df_all[['commit','path']].groupby('commit')['path'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate through commit data and call the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import csv\n",
    "#inspect all repos and commits\n",
    "fields = ['line','function','package','class','repo','path', 'author', 'commit','code']\n",
    "to_disk = \"../data/repo_function_matched.csv\"\n",
    "with open(to_disk, \"w\", encoding=\"utf8\") as newfile:\n",
    "    writer = csv.DictWriter(newfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    \n",
    "for r in repos:\n",
    "    try:\n",
    "        repo_matcher(r, commits, paths, to_disk)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
